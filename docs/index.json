[{"content":"The regular expression is an algebraic expression for text search strings. It defines a representation to match a set of strings.\nThe basic format of regex is /\u0026lt;regex\u0026gt;/ e.g. /love/ matches lover, but not Love. Regex is case-sensitive.\nDisjunction of A character The [] is used to represent character disjunction (or). e.g. /l[ai]ke/ matches like, lack. The place at the second character is either i or a.\nMatch a range of characters Inside the [], using - can represent a range of characters, for instance. [a-z] is a lower case letter from a to z. /[A-Za-z0-9]/ represents numerical digits, upper case letters, and lower case letters.\nIndicate not some characters Inside [] using, the caret ^ right after [ can be used to match characters that are not the specific set of characters. e.g. \\[^a]\\ matches all characters except for a. e.g. /[^0-9]/ matches 90 \u0026lt;u\u0026gt;k\u0026lt;/u\u0026gt;ilogram. (here only underscore the first matched character).\nTo be mentioned, if the caret ^ is not right after [ inside of square brackets, it is only a caret! When outside of square brackets, /\\^/ matches ^\nThe counters and wildcard expression Counters The counter is being used to indicate how many times the pattern before them should repeat in a matched token.\nExpression Meaning ? 0 or 1 times + 1 or more times * 0 or more times {,} at least l times, at most u times. If l and u leave empty, e.g.{2,}/{,2}indicates at least/most two times. {} the pattern should repeat exact k times The counter should be placed at the end of the pattern. e.g. /a+/ matches apple, aaa.\nWildcard expression (通配符) /./ is the wildcard expression, which represents any characters, except the carriage return.\nAnchors Expression Meaning ^ Start of the line anchor $ End of the line \\b word boundary \\B not a word boundary Disjunction, Grouping, and Precedence disjunction | is called the disjunction operator, also called the pipe symbol, which means \u0026ldquo;or\u0026rdquo;.\n/cat|dog/ matches the word cat ,dog.\ngroup Group is being indicated using (), which is a string of characters that behaves atomically like a single character.\nFor instance /the(re)?/ matches the and there.\nprecedence () counters sequences and anchor disjunction | ","permalink":"https://cyfine.github.io/posts/regex/","summary":"The regular expression is an algebraic expression for text search strings. It defines a representation to match a set of strings.\nThe basic format of regex is /\u0026lt;regex\u0026gt;/ e.g. /love/ matches lover, but not Love. Regex is case-sensitive.\nDisjunction of A character The [] is used to represent character disjunction (or). e.g. /l[ai]ke/ matches like, lack. The place at the second character is either i or a.\nMatch a range of characters Inside the [], using - can represent a range of characters, for instance.","title":"Regular Expression"},{"content":"Summary Research Objective There are recent found that, from a large neural trained network, we can prune and obtain a small sub network (even 90% of the parameters is being pruned), without compromising the performance. It natural to think that, if we could have a way, train a small network from the scratch, also obtains similar performance as the large network, saving the energy for training. According to current experience, a pruned sparse network is hard to train from start.\nProblem Statement The author proposed the Lottery Ticker Hypothesis, stating:\nA randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.\nMethods The training is procedures are\nInit a large dense network, init with $\\theta_0$ Train the network Find a mask, prune with Least Weight Magnitude Rest the weight to $\\theta_0$, and apply the mask Repeat above steps, prune iteratively.\nContribution The interesting finding of the lottery ticket hypothesis, that existence of sub network reach better performance and higher accuracy than the original network within fewer iterations. The case under the constraint that the parameter should be the same as the initial param of untrained dense network. Evaluation Conclusion Notes References Background:\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXivpreprint arXiv:1503.02531, 2015. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pp. 1135–1143,2015. Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. arXiv preprint, 2017. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. arXiv preprint arXiv:1707.06342, 2017. ","permalink":"https://cyfine.github.io/posts/paper/the-lottery-ticket-hypothesis/","summary":"Summary Research Objective There are recent found that, from a large neural trained network, we can prune and obtain a small sub network (even 90% of the parameters is being pruned), without compromising the performance. It natural to think that, if we could have a way, train a small network from the scratch, also obtains similar performance as the large network, saving the energy for training. According to current experience, a pruned sparse network is hard to train from start.","title":"The Lottery Ticket Hypothesis"},{"content":"1. Abstract When using an over-parameterized model, the author found the model can be pruned without losing the most accuracy.\nThe author identifies the essential weight using LWM to obtain the mask. For pruned model, train on source domain with regularization. Then, transfer the model to the target domain, freeze the un-pruned parameter on the source domain, and train only the pruned parameter on the target domain. 2. Contribution Very first using pruning in transfer learning. First use transfer learning to solve ODE and PDE. Using pruning with regularization to achieve SOTA as claimed 2.1. Idea Using prune techniques in transfer learning. For a large model, it considers prune as the optimal weight search. The unpruned part of the source domain\u0026rsquo;s model is frozen when training on the target domain. Utilize the rest of the model capacity (the pruned part of the model)on the target domain. 3. Proposed Framework 3.1. Pruning Prune using LWM, only preserve the largest K weights of the original source trained model. The rest of the weights are being pruned. $$ m^{i}= \\begin{cases}1, \u0026amp; \\text { if }\\left|w^{i}\\right|\u0026gt;w_{\\kappa} \\\\ 0, \u0026amp; \\text { otherwise }\\end{cases} $$\n3.2. Allocation After pruning the model with the LWM $k$-th largest mask, on the source domain, train the pruned model with regularization. The regularization method being used is\n$$ \\Omega(\\mathbf{w})=\\lambda||\\mathbf{w}-\\mathbf{w}_{S}||^{2}=\\lambda \\sum_{i=1}^{D}||w^{i}-w_{S}^{i}||^{2} $$ This is the starting point regularization, encourages the transferred target weight should be close to the initial source weight to avoid catastrophic forgetting.\nIt is because some important weights may be driven far away from their initial $w_S$\n3.3. Calibration This step is train on the target domain. First, unpruned weight train on the source domain is frozen. The parameter update is being applied on the pruned weight. The main idea maybe to utilize the unused capacity of the model.\n","permalink":"https://cyfine.github.io/posts/paper/pac-net-a-model-pruning-approach/","summary":"1. Abstract When using an over-parameterized model, the author found the model can be pruned without losing the most accuracy.\nThe author identifies the essential weight using LWM to obtain the mask. For pruned model, train on source domain with regularization. Then, transfer the model to the target domain, freeze the un-pruned parameter on the source domain, and train only the pruned parameter on the target domain. 2. Contribution Very first using pruning in transfer learning.","title":"Pac Net a Model Pruning  Approach"},{"content":"Assume we have a box filled with balls in two colors, black and white. We do not know the ratio of black to white balls, and we take a simple random sampling with replacement. The color of the ball is a random variable $X$. The distribution of the balls in the box is represented by the percentage of the black balls, denoted as $\\theta$. Assume sampling ten times, and six times the ball is drawn black. We assume the $\\theta$ is 0.6 intuitively because we believe it is the most likely distribution to obtain the experiment outcome. In the words of MLE, we are finding the given $\\theta$ that maximizes the probability of the experiment outcome.\n$$ \\begin{aligned} \\theta_{M L E}\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\ p(X \\mid \\theta)\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\ \\ln (p(X \\mid \\theta))\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln \\left(\\theta^{6}(1-\\theta)^{4}\\right)\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ 6 \\ln \\theta+4 \\ln (1-\\theta)\\\\ \\end{aligned} $$ $$\\text { let: } \\ln ^{\\prime}(p(X \\mid \\theta))=0, \\ \\theta=0.6$$ The MLE has no clue about the prior distribution of the parameters $\\theta$. MLE considers no occasionality of the experiment when the data sample is insufficient. Maybe, the real distribution is 90% of the balls are white balls, and we are just lucky to draw six times black balls in the 10-time sampling. Suppose we have some prior information about the real distribution. For example, if our prior information is that the parameter $\\theta\\sim N(0.1,1)$. According to our prior, $p(\\theta=0.6)$ will be very small. If our case in the MLE is exceptional and our prior is close to the real distribution of $\\theta$, then the MAP will give a better estimation. The term $\\ln(p(\\theta))$ is like a regularization term, preventing a possible faulty estimation due to exceptional observation of the data. Under the case that the prior distribution of $\\theta$ is close to real or the observation of $X$ is sufficient to prevent the exceptional case, MAP and MLE will be similar. $$ \\begin{aligned} \\theta_{\\text {MAP }} \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ p(\\theta \\mid X) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(\\theta \\mid X))\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(X \\mid \\theta) \\cdot p(\\theta)) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(X \\mid \\theta))+\\ln (p(\\theta)) \\end{aligned} $$ At line 3 of the above equation, the $p(X)$ is omitted, as doing so does not affect the result. The $MAP$ has one extra $\\ln(p(\\theta))$ term.\nNote, here is an inaccuracy:\nAccording to our prior, $p(\\theta=0.6)$ will be very small.\nAs the $\\theta$ follows Gaussian distribution, it is a continuous variable,thus $p(\\theta=0.6)$ is meaningless, still, the p.d.f gives smaller value, which aligns its functionality as a \u0026ldquo;weighting\u0026rdquo; of MLE.\n","permalink":"https://cyfine.github.io/posts/maximum-likelyhood-estimation-and-maximum-a-posteriori/","summary":"Given observed data, MLE and MAP find the parameters of the distribution that maximize the probability of observed data. Compares to MLE, MAP considers prior information of theta while MLE does not","title":"Maximum Likelihood Estimation and Maximum A Posterior"},{"content":"The Entropy and Cross-Entropy Self Information Considering the scenario that we transport information of a random variable $X$ having different states. The information can be represented using bits. For instance, if a random variable has two states, it can be represented using a single bit. The amount of bits representing the random variable $X$ is $\\log_{2}{2}$. Similarly, if $X$ has three states, the number of bits representing $X$ will be $\\log_{2}{8}=3$.\nThe actual information contained in each state is correlated to its probability. From the perspective of how many bits to be transferred, the self information is: $$I(x) = -\\log_2{p(x)}$$\nMore precise definition from Shannon: Claude Shannon\u0026rsquo;s definition of self-information was chosen to meet several axioms:\nAn event with probability 100% is perfectly unsurprising and yields no information. The less probable an event is, the more surprising it is and the more information it yields. If two independent events are measured separately, the total amount of information is the sum of the self-informations of the individual events. Entropy and Cross Entropy Encoding each value using the number of states of random variables is assuming that each state has an equal probability, complying with uniform distribution. If $x_i$ has a different probability to appear, transmitting $\\log_2{n}$ bits for each state is not an optimal solution. For example, all possible values of $X$ are $X = \\{x_1, x_2, x_3\\}$, and the probability distribution of $X$ is $$ p(x_1)=0.5, p(x_2)=0.25,p(x_3)=0.25$$ We need to use $\\log_{2}{3}$, approximately 1.68 bits to encode each state. However, if we use 1 bit for $x_1$, and 2 bits for $x_2$ and $x_3$ (using more bits to encode the value with less probability, vice versa), on average the number of bits that we need to transmit are $$p(x_1)\\log_2{\\frac{1}{p(x_1)}} + p(x_2)\\log_2{\\frac{1}{p(x_2)}} +p(x_3)\\log_2{\\frac{1}{p(x_3)}}$$ $$0.5\\times1 + 2\\times(0.25\\times 2) = 1.5$$ which is less than 1.68, even though we need to use more bits to represent for some states. The entropy, measuring how chaos, a lower bound indication of information of the random variable with probability distribution $p$ of its values is defined as follows: $$H(p) = -\\sum_{i\\in N}p(x_i)\\log_{2}{p(x_i )}, \\ X=\\{x_1, x_2,\\dots, x_N\\}$$\nIn the actual transmission of the random variable $X$, we use another distribution $q$ guiding us to encode each value of random variable $X$. We are trying to approach the real distribution of random variable $X$ with $q$ to obtain better encoding with fewer bits transferred. The measurement of how similar two distributions is cross-entropy. The cross-entropy is being represented as: $$ H(p|q) = -\\sum_{i\\in N}p(x_i)\\log_2{q(x_i)} $$\nKullback-Leibler Divergence (KL Divergence) Extending from cross-entropy, the KL divergence is: $$\\begin{aligned} D_{KL}(p|q) \u0026amp;= H(p|q) - H(p) \\\\\\ \u0026amp;= -\\sum_{i \\in N}p(x_i)\\log_2{q(x_i)} - \\sum_{i\\in N}p(x_i)\\log_2{p(x_i)} \\\\ \u0026amp;= -\\sum_{i \\in N}p(x_i)\\log_2{\\frac{q(x_i)}{p(x_i)}} \\end{aligned}$$ Measuring if using distribution $q$ to approximate $p$, the extra information loss/overhead. So, the smaller the KL divergence is, the more similar the two distributions are.\nProperties Positive Definiteness $$D_{K L}(p | q) \\geq 0$$\nBy Gibbs Inequality: if $\\sum_{i=1}^{n} p_{i}=\\sum_{i=1}^{n} q_{i}=1$ and $p_{i}, q_{i} \\in(0,1]$, then $$-\\sum_{i}^{n} p_{i} \\log p_{i} \\leq-\\sum_{i}^{n} p_{i} \\log q_{i}$$ The equal signs takes if and only if $p_{i}=q_{i} \\forall i$\nAsymmetric $$D(p | q) \\neq D(q | p)$$\nReferences KL Divergence Explained Self-information - Wikipedia ","permalink":"https://cyfine.github.io/posts/a-briefing-of-cross-entropy-and-kl-divergence/","summary":"Cross entropy and KL divergence measure how different two distributions are. In information theory,  KL Divergence measures the information loss / extra information when using distribution q to approximate the distribution p KL(p|q)","title":"A Briefing of Cross-Entropy and KL Divergence"},{"content":"Introduction We may measure a random variable indirectly. However, the indirect measurement is affected by the noise following a certain distribution. Thus, the measurement is represented by a random variable that follows a certain distribution. For example, we want to measure $\\theta$, the temperate of the stomach, and it can be measured by measuring the mouth\u0026rsquo;s temperature $Y$. $$Y = \\theta + W$$ $W$ is the noise that follows the normal distribution $N\\sim(0,\\sigma)$. If $Y$ is very informative regarding $\\theta$, $Y$ with different $\\theta$ should be distinguishable. If $\\sigma$ is large, the distribution of $Y$ is plattered, and the $Y$ with different $\\theta$ will overlap largely, which is less distinguishable. The less indicative $Y$ contains less information about $\\theta$. How informative the random variable $Y$ is regarding $\\theta$ can be measured using Fisher information.\n$$ I_{Y}(\\theta)=E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(Y ; \\theta)\\right)^{2}\\right] $$\nReferences What is Fisher information? - YouTube ","permalink":"https://cyfine.github.io/posts/fisher-information/","summary":"Introduction We may measure a random variable indirectly. However, the indirect measurement is affected by the noise following a certain distribution. Thus, the measurement is represented by a random variable that follows a certain distribution. For example, we want to measure $\\theta$, the temperate of the stomach, and it can be measured by measuring the mouth\u0026rsquo;s temperature $Y$. $$Y = \\theta + W$$ $W$ is the noise that follows the normal distribution $N\\sim(0,\\sigma)$.","title":"Fisher Information"}]