[{"content":"Assume we have a box filled with balls in two colors, black and white. We do not know the ratio of black to white balls, and we take a simple random sampling with replacement. The color of the ball is a random variable $X$. The distribution of the balls in the box is represented by the percentage of the black balls, denoted as $\\theta$. Assume sampling ten times, and six times the ball is drawn black. We assume the $\\theta$ is 0.6 intuitively because we believe it is the most likely distribution to obtain the experiment outcome. In the words of MLE, we are finding the given $\\theta$ that maximizes the probability of the experiment outcome.\n$$ \\begin{aligned} \\theta_{M L E}\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\ p(X \\mid \\theta)\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\ \\ln (p(X \\mid \\theta))\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln \\left(\\theta^{6}(1-\\theta)^{4}\\right)\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ 6 \\ln \\theta+4 \\ln (1-\\theta)\\\\ \\end{aligned} $$ $$\\text { let: } \\ln ^{\\prime}(p(X \\mid \\theta))=0, \\ \\theta=0.6$$ The MLE has no clue about the prior distribution of the parameters $\\theta$. MLE considers no occasionality of the experiment when the data sample is insufficient. Maybe, the real distribution is 90% of the balls are white balls, and we are just lucky to draw six times black balls in the 10-time sampling. Suppose we have some prior information about the real distribution. For example, if our prior information is that the parameter $\\theta\\sim N(0.1,1)$. According to our prior, $p(\\theta=0.6)$ will be very small. If our case in the MLE is exceptional and our prior is close to the real distribution of $\\theta$, then the MAP will give a better estimation. The term $\\ln(p(\\theta))$ is like a regularization term, preventing a possible faulty estimation due to exceptional observation of the data. Under the case that the prior distribution of $\\theta$ is close to real or the observation of $X$ is sufficient to prevent the exceptional case, MAP and MLE will be similar. $$ \\begin{aligned} \\theta_{\\text {MAP }} \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ p(\\theta \\mid X) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(\\theta \\mid X))\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(X \\mid \\theta) \\cdot p(\\theta)) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}\\ \\ln (p(X \\mid \\theta))+\\ln (p(\\theta)) \\end{aligned} $$ At line 3 of the above equation, the $p(X)$ is omitted, as doing so does not affect the result. The $MAP$ has one extra $\\ln(p(\\theta))$ term.\n","permalink":"https://cyfine.github.io/posts/maximum-likelyhood-estimation-and-maximum-a-posteriori/","summary":"Given observed data, MLE and MAP find the parameters of the distribution that maximize the probability of observed data. Compares to MLE, MAP considers prior information of theta while MLE does not","title":"Maximum Likelihood Estimation and Maximum A Posterior"},{"content":"The Entropy and Cross-Entropy Self Information Considering the scenario that we transport information of a random variable $X$ having different states. The information can be represented using bits. For instance, if a random variable has two states, it can be represented using a single bit. The amount of bits representing the random variable $X$ is $\\log_{2}{2}$. Similarly, if $X$ has three states, the number of bits representing $X$ will be $\\log_{2}{8}=3$.\nThe actual information contained in each state is correlated to its probability. From the perspective of how many bits to be transferred, the self information is: $$I(x) = -\\log_2{p(x)}$$\nMore precise definition from Shannon: Claude Shannon\u0026rsquo;s definition of self-information was chosen to meet several axioms:\nAn event with probability 100% is perfectly unsurprising and yields no information. The less probable an event is, the more surprising it is and the more information it yields. If two independent events are measured separately, the total amount of information is the sum of the self-informations of the individual events. Entropy and Cross Entropy Encoding each value using the number of states of random variables is assuming that each state has an equal probability, complying with uniform distribution. If $x_i$ has a different probability to appear, transmitting $\\log_2{n}$ bits for each state is not an optimal solution. For example, all possible values of $X$ are $X = \\{x_1, x_2, x_3\\}$, and the probability distribution of $X$ is $$ p(x_1)=0.5, p(x_2)=0.25,p(x_3)=0.25$$ We need to use $\\log_{2}{3}$, approximately 1.68 bits to encode each state. However, if we use 1 bit for $x_1$, and 2 bits for $x_2$ and $x_3$ (using more bits to encode the value with less probability, vice versa), on average the number of bits that we need to transmit are $$p(x_1)\\log_2{\\frac{1}{p(x_1)}} + p(x_2)\\log_2{\\frac{1}{p(x_2)}} +p(x_3)\\log_2{\\frac{1}{p(x_3)}}$$ $$0.5\\times1 + 2\\times(0.25\\times 2) = 1.5$$ which is less than 1.68, even though we need to use more bits to represent for some states. The entropy, measuring how chaos, a lower bound indication of information of the random variable with probability distribution $p$ of its values is defined as follows: $$H(p) = -\\sum_{i\\in N}p(x_i)\\log_{2}{p(x_i )}, \\ X=\\{x_1, x_2,\\dots, x_N\\}$$\nIn the actual transmission of the random variable $X$, we use another distribution $q$ guiding us to encode each value of random variable $X$. We are trying to approach the real distribution of random variable $X$ with $q$ to obtain better encoding with fewer bits transferred. The measurement of how similar two distributions is cross-entropy. The cross-entropy is being represented as: $$ H(p|q) = -\\sum_{i\\in N}p(x_i)\\log_2{q(x_i)} $$\nKullback-Leibler Divergence (KL Divergence) Extending from cross-entropy, the KL divergence is: $$\\begin{aligned} D_{KL}(p|q) \u0026amp;= H(p|q) - H(p) \\\\\\ \u0026amp;= -\\sum_{i \\in N}p(x_i)\\log_2{q(x_i)} - \\sum_{i\\in N}p(x_i)\\log_2{p(x_i)} \\\\ \u0026amp;= -\\sum_{i \\in N}p(x_i)\\log_2{\\frac{q(x_i)}{p(x_i)}} \\end{aligned}$$ Measuring if using distribution $q$ to approximate $p$, the extra information loss/overhead. So, the smaller the KL divergence is, the more similar the two distributions are.\nProperties Positive Definiteness $$D_{K L}(p | q) \\geq 0$$\nBy Gibbs Inequality: if $\\sum_{i=1}^{n} p_{i}=\\sum_{i=1}^{n} q_{i}=1$ and $p_{i}, q_{i} \\in(0,1]$, then $$-\\sum_{i}^{n} p_{i} \\log p_{i} \\leq-\\sum_{i}^{n} p_{i} \\log q_{i}$$ The equal signs takes if and only if $p_{i}=q_{i} \\forall i$\nAsymmetric $$D(p | q) \\neq D(q | p)$$\nReferences KL Divergence Explained Self-information - Wikipedia ","permalink":"https://cyfine.github.io/posts/a-briefing-of-cross-entropy-and-kl-divergence/","summary":"Cross entropy and KL divergence measure how different two distributions are. In information theory,  KL Divergence measures the information loss / extra information when using distribution q to approximate the distribution p KL(p|q)","title":"A Briefing of Cross-Entropy and KL Divergence"},{"content":"Introduction We may measure a random variable indirectly. However, the indirect measurement is affected by the noise following a certain distribution. Thus, the measurement is represented by a random variable that follows a certain distribution. For example, we want to measure $\\theta$, the temperate of the stomach, and it can be measured by measuring the mouth\u0026rsquo;s temperature $Y$. $$Y = \\theta + W$$ $W$ is the noise that follows the normal distribution $N\\sim(0,\\sigma)$. If $Y$ is very informative regarding $\\theta$, $Y$ with different $\\theta$ should be distinguishable. If $\\sigma$ is large, the distribution of $Y$ is plattered, and the $Y$ with different $\\theta$ will overlap largely, which is less distinguishable. The less indicative $Y$ contains less information about $\\theta$. How informative the random variable $Y$ is regarding $\\theta$ can be measured using Fisher information.\n$$ I_{Y}(\\theta)=E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(Y ; \\theta)\\right)^{2}\\right] $$\nReferences What is Fisher information? - YouTube ","permalink":"https://cyfine.github.io/posts/fisher-information/","summary":"Introduction We may measure a random variable indirectly. However, the indirect measurement is affected by the noise following a certain distribution. Thus, the measurement is represented by a random variable that follows a certain distribution. For example, we want to measure $\\theta$, the temperate of the stomach, and it can be measured by measuring the mouth\u0026rsquo;s temperature $Y$. $$Y = \\theta + W$$ $W$ is the noise that follows the normal distribution $N\\sim(0,\\sigma)$.","title":"Fisher Information"}]