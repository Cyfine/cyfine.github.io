<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Carter&#39;s Blog</title>
    <link>https://cyfine.github.io/posts/</link>
    <description>Recent content in Posts on Carter&#39;s Blog</description>
    <image>
      <url>https://cyfine.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://cyfine.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 02 Aug 2022 11:31:28 +0800</lastBuildDate><atom:link href="https://cyfine.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pac Net a Model Pruning  Approach</title>
      <link>https://cyfine.github.io/posts/pac-net-a-model-pruning-approach/</link>
      <pubDate>Tue, 02 Aug 2022 11:31:28 +0800</pubDate>
      
      <guid>https://cyfine.github.io/posts/pac-net-a-model-pruning-approach/</guid>
      <description>1. Abstract When using an over-parameterized model, the author found the model can be pruned without losing the most accuracy.
The author identifies the essential weight using LWM to obtain the mask. For pruned model, train on source domain with regularization. Then, transfer the model to the target domain, freeze the un-pruned parameter on the source domain, and train only the pruned parameter on the target domain. 2. Contribution Very first using pruning in transfer learning.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation and Maximum A Posterior</title>
      <link>https://cyfine.github.io/posts/maximum-likelyhood-estimation-and-maximum-a-posteriori/</link>
      <pubDate>Fri, 22 Jul 2022 20:02:09 +0800</pubDate>
      
      <guid>https://cyfine.github.io/posts/maximum-likelyhood-estimation-and-maximum-a-posteriori/</guid>
      <description>Given observed data, MLE and MAP find the parameters of the distribution that maximize the probability of observed data. Compares to MLE, MAP considers prior information of theta while MLE does not</description>
    </item>
    
    <item>
      <title>A Briefing of Cross-Entropy and KL Divergence</title>
      <link>https://cyfine.github.io/posts/a-briefing-of-cross-entropy-and-kl-divergence/</link>
      <pubDate>Tue, 19 Jul 2022 19:33:24 +0800</pubDate>
      
      <guid>https://cyfine.github.io/posts/a-briefing-of-cross-entropy-and-kl-divergence/</guid>
      <description>CE and KL divergence are important concepts to measure how different two distributions are from each other on a statistics point of view. Also, viewing from information theory, KL Divergence measures the information loss / extra information when using distribution q to approximate the distribution p KL(p|q)</description>
    </item>
    
    <item>
      <title>Fisher Information</title>
      <link>https://cyfine.github.io/posts/fisher-information/</link>
      <pubDate>Tue, 19 Jul 2022 19:32:13 +0800</pubDate>
      
      <guid>https://cyfine.github.io/posts/fisher-information/</guid>
      <description>Introduction We may measure a random variable indirectly. However, the indirect measurement is affected by the noise following a certain distribution. Thus, the measurement is represented by a random variable that follows a certain distribution. For example, we want to measure $\theta$, the temperate of the stomach, and it can be measured by measuring the mouth&amp;rsquo;s temperature $Y$. $$Y = \theta + W$$ $W$ is the noise that follows the normal distribution $N\sim(0,\sigma)$.</description>
    </item>
    
  </channel>
</rss>
